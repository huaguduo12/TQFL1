import os
import re
import base64
import requests
from urllib.parse import unquote
from github import Github

# --- 1. ä»ç¯å¢ƒå˜é‡è·å–é…ç½® ---
GITHUB_TOKEN = os.getenv("MY_GITHUB_TOKEN")
REPO_NAME = os.getenv("REPO_NAME")
FILE_PATH = os.getenv("FILE_PATH")
WEBPAGE_URLS = os.getenv("WEBPAGE_URLS", "").strip().splitlines()

# å¯é€‰ï¼šå›½å®¶/åœ°åŒºä»£ç æ’åº, å¦‚æœç¯å¢ƒå˜é‡ä¸ºç©ºæˆ–ä¸å­˜åœ¨, åˆ™ä½¿ç”¨é»˜è®¤å€¼
COUNTRY_ORDER_STR = os.getenv("COUNTRY_ORDER") or "HK,SG,JP,TW,KR,US,CA,AU,GB,FR,IT,NL,DE,RU,PL"
COUNTRY_ORDER = [code.strip() for code in COUNTRY_ORDER_STR.split(',')]

# å¯é€‰ï¼šæ¯ä¸ªå›½å®¶/åœ°åŒºä¿ç•™çš„é“¾æ¥æ•°é‡, å¦‚æœç¯å¢ƒå˜é‡ä¸ºç©ºæˆ–ä¸å­˜åœ¨, åˆ™ä½¿ç”¨é»˜è®¤å€¼ "20"
LINKS_PER_COUNTRY = int(os.getenv("LINKS_PER_COUNTRY") or "20")

# <<< æ–°å¢åŠŸèƒ½åŒº START >>>
# å¯é€‰ï¼šä¸ºå›½å®¶ä»£ç æ·»åŠ è‡ªå®šä¹‰å‰åç¼€
LINK_PREFIX = os.getenv("LINK_PREFIX", "ğŸ’®")
LINK_SUFFIX = os.getenv("LINK_SUFFIX", "ğŸ’–")
# <<< æ–°å¢åŠŸèƒ½åŒº END >>>


# --- 2. æ£€æŸ¥æ ¸å¿ƒç¯å¢ƒå˜é‡ ---
if not GITHUB_TOKEN or not REPO_NAME or not FILE_PATH:
    print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ GitHub ç¯å¢ƒå˜é‡ (MY_GITHUB_TOKEN, REPO_NAME, FILE_PATH)")
    exit(1)
if not WEBPAGE_URLS:
    print("é”™è¯¯: ç¯å¢ƒå˜é‡ WEBPAGE_URLS æœªè®¾ç½®æˆ–ä¸ºç©ºã€‚")
    exit(1)

# --- 3. å›½å®¶/åœ°åŒºåç§°åˆ°ä»£ç çš„æ˜ å°„ ---
COUNTRY_MAPPING = {
    "é¦™æ¸¯": "HK", "æ¾³é—¨": "MO", "å°æ¹¾": "TW", "éŸ©å›½": "KR", "æ—¥æœ¬": "JP",
    "æ–°åŠ å¡": "SG", "ç¾å›½": "US", "è‹±å›½": "GB", "æ³•å›½": "FR", "å¾·å›½": "DE",
    "åŠ æ‹¿å¤§": "CA", "æ¾³å¤§åˆ©äºš": "AU", "æ„å¤§åˆ©": "IT", "è·å…°": "NL", "æŒªå¨": "NO",
    "èŠ¬å…°": "FI", "ç‘å…¸": "SE", "ä¸¹éº¦": "DK", "ç«‹Ñ‚Ğ¾Ğ²": "LT", "ä¿„ç½—æ–¯": "RU",
    "å°åº¦": "IN", "åœŸè€³å…¶": "TR", "æ·å…‹": "CZ", "çˆ±æ²™å°¼äºš": "EE", "æ‹‰è„±ç»´äºš": "LV",
    "éƒ½æŸæ—": "IE", "è¥¿ç­ç‰™": "ES", "å¥¥åœ°åˆ©": "AT", "ç½—é©¬å°¼äºš": "RO", "æ³¢å…°": "PL"
}

# --- 4. æ ¸å¿ƒå¤„ç†å‡½æ•° ---

def extract_vless_links(decoded_content):
    regex = re.compile(r'vless://[a-zA-Z0-9\-]+@([^:]+):(\d+)\?[^#]+#([^\n\r]+)')
    links = []
    for match in regex.finditer(decoded_content):
        ip = match.group(1)
        port = match.group(2)
        country_name_raw = unquote(match.group(3).strip())
        country_code = "UNKNOWN"
        for name, code in COUNTRY_MAPPING.items():
            if name in country_name_raw:
                country_code = code
                break
        else:
            code_match = re.search(r'([A-Z]{2})', country_name_raw)
            if code_match:
                country_code = code_match.group(1)

        if country_code != "UNKNOWN":
            # <<< ä¿®æ”¹ç‚¹: åº”ç”¨å‰åç¼€ >>>
            formatted_link = f"{ip}:{port}#{LINK_PREFIX}{country_code}{LINK_SUFFIX}"
            links.append({"link": formatted_link, "country_code": country_code})
    return links

def extract_plain_text_links(plain_content):
    regex = re.compile(r'([^:]+:\d+)#([A-Z]{2})')
    links = []
    for match in regex.finditer(plain_content):
        link_part = match.group(1)
        country_code = match.group(2)
        
        # <<< ä¿®æ”¹ç‚¹: åº”ç”¨å‰åç¼€ >>>
        formatted_link = f"{link_part}#{LINK_PREFIX}{country_code}{LINK_SUFFIX}"
        links.append({"link": formatted_link, "country_code": country_code})
    return links

def process_subscription_url(url):
    print(f"æ­£åœ¨å¤„ç† URL: {url}")
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        raw_content = response.text
        try:
            base64_content = "".join(raw_content.split())
            decoded_bytes = base64.b64decode(base64_content)
            try:
                decoded_text = decoded_bytes.decode('utf-8')
            except UnicodeDecodeError:
                decoded_text = decoded_bytes.decode('gbk', 'ignore')
            print("  > æ£€æµ‹åˆ° Base64 æ ¼å¼ï¼Œä½¿ç”¨ vless è§£æå™¨ã€‚")
            return extract_vless_links(decoded_text)
        except Exception:
            print("  > æ£€æµ‹åˆ°çº¯æ–‡æœ¬æ ¼å¼ï¼Œä½¿ç”¨çº¯æ–‡æœ¬è§£æå™¨ã€‚")
            return extract_plain_text_links(raw_content)
    except requests.RequestException as e:
        print(f"  > è·å– URL å†…å®¹å¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"  > å¤„ç†å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

def filter_and_sort_links(all_links, country_order, limit):
    grouped_links = {}
    for link_info in all_links:
        code = link_info['country_code']
        if code not in grouped_links:
            grouped_links[code] = []
        grouped_links[code].append(link_info['link'])
    
    sorted_and_filtered_links = []
    for country_code in country_order:
        if country_code in grouped_links:
            unique_links = list(dict.fromkeys(grouped_links[country_code]))
            sorted_and_filtered_links.extend(unique_links[:limit])
            
    return sorted_and_filtered_links

def write_to_github(content):
    if not content:
        print("æ²¡æœ‰ç”Ÿæˆä»»ä½•å†…å®¹ï¼Œå·²è·³è¿‡å†™å…¥ GitHubã€‚")
        return
    try:
        g = Github(GITHUB_TOKEN)
        repo = g.get_repo(REPO_NAME)
        try:
            file = repo.get_contents(FILE_PATH, ref="main")
            repo.update_file(path=FILE_PATH, message="Update subscription links", content=content, sha=file.sha, branch="main")
            print(f"æ–‡ä»¶ {FILE_PATH} å·²åœ¨ä»“åº“ {REPO_NAME} ä¸­æˆåŠŸæ›´æ–°ã€‚")
        except Exception:
            repo.create_file(path=FILE_PATH, message="Create subscription links file", content=content, branch="main")
            print(f"æ–‡ä»¶ {FILE_PATH} å·²åœ¨ä»“åº“ {REPO_NAME} ä¸­æˆåŠŸåˆ›å»ºã€‚")
    except Exception as e:
        print(f"å†™å…¥ GitHub æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def main():
    print("å¼€å§‹æ‰§è¡Œè®¢é˜…é“¾æ¥å¤„ç†ä»»åŠ¡...")
    all_extracted_links = []
    for url in WEBPAGE_URLS:
        if url:
            links = process_subscription_url(url)
            if links:
                all_extracted_links.extend(links)
    
    print(f"\nä»æ‰€æœ‰æºå…±æå–äº† {len(all_extracted_links)} ä¸ªæœ‰æ•ˆé“¾æ¥ã€‚")

    if not all_extracted_links:
        print("æœªèƒ½ä»ä»»ä½•æºæå–åˆ°é“¾æ¥ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
        return

    final_links = filter_and_sort_links(all_extracted_links, COUNTRY_ORDER, LINKS_PER_COUNTRY)
    print(f"ç»è¿‡æ’åºå’Œç­›é€‰åï¼Œæœ€ç»ˆä¿ç•™ {len(final_links)} ä¸ªé“¾æ¥ã€‚")
    
    final_content = "\n".join(final_links)
    write_to_github(final_content)

if __name__ == "__main__":
    main()
